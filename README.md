# LLM Feedback Rater Agreement

This project evaluates the consistency of formative feedback scoring in clinical education using both human annotators and a large language model (GPT-4). The goal is to assess how well structured rubrics can support reliable evaluationâ€”and whether AI tools can serve as consistent, pedagogically aligned raters.

## ğŸ“ Overview

We computed:
- **Krippendorffâ€™s Alpha** to measure inter-rater agreement across three trained human annotators.
- **Cohenâ€™s Kappa** to measure alignment between GPT-4 scores and the human majority vote.

The rubric applied scored feedback across 7 traits (e.g., Constructive Tone, Actionable Suggestions, Student Learning) using binary scoring (0 = does not meet criterion, 1 = meets criterion).

## ğŸ“Š Results

- **Krippendorffâ€™s Alpha** (Human-Human Agreement): `0.575`
- **Cohenâ€™s Kappa** (GPT-4 vs. Human Majority): `0.738`

Traits with observable anchors (e.g., tone, specificity) had stronger agreement. Traits requiring interpretive judgment (e.g., learning value) showed more variability.

## ğŸ“ Files

- `Cleaned_Long-Format_Scores.csv` â€“ Long-format dataset with rater, trait, score, and feedback ID.
- `analysis.py` â€“ Script to compute Krippendorffâ€™s Alpha and Cohenâ€™s Kappa.
- `README.md` â€“ Project documentation.

## ğŸ”§ Dependencies

- Python 3.x
- `pandas`
- `numpy`
- `scikit-learn`
- `krippendorff`

Install with:

```bash
pip install pandas scikit-learn krippendorff
